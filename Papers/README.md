# Reading list

## Introduction

- [ ] Linguistic unit discovery from multi-modal inputs in unwritten languages: Summary of the "Speaking Rosetta" JSALT 2017 Workshop. [[paper](https://arxiv.org/pdf/1802.05092.pdf)].[[note](https://github.com/YimingXu1/multimodel-learning-notes/blob/main/Papers/Linguistic%20Unit%20Discovery.md)].

## ASR 

- [ ] Breaking the unwritten kanguage barrier: The Bulb project.[[Paper](https://www.sciencedirect.com/science/article/pii/S1877050916300370)].

## Speech&Image

- [ ] Align or attend? Toward More Efficient and Accurate Spoken Word Discovery Using Speech-to-Image Retrieval.[[paper](http://homepage.tudelft.nl/f7h35/papers/icassp21.3.pdf)].
- [ ] Learning to Recognise Words using Visually Grounded Speech. [[paper](http://homepage.tudelft.nl/f7h35/papers/iscas2021.1.pdf)].
- [ ] Image2speech: Automatically generating audio descriptions of images. [[paper](http://odettescharenborg.ruhosting.nl/wp-content/uploads/2015/02/hasegawajohnson_isga18.pdf)].

## Text&Image

- [x] Modeling Text with Graph Convolutional Network for Cross-Modal Information Retrieval. [[paper](https://arxiv.org/pdf/1802.00985.pdf)].[[note](https://github.com/YimingXu1/multimodel-learning-notes/blob/main/Papers/Linguistic%20Unit%20Discovery.md)].

- [x] Cross-modal Scene Graph Matching for Relationship-aware Image-Text Retrieval. [[paper](https://arxiv.org/pdf/1910.05134.pdf)].[[note](https://github.com/YimingXu1/multimodel-learning-notes/blob/main/Papers/SGM-CMIR.md)].
- [ ] Cross-modal Graph Matching Network for Image-text Retrieval. [[paper](https://dl.acm.org/doi/pdf/10.1145/3499027?casa_token=wR_OBjzXlj0AAAAA:4nyCycmHj9EqxFLoxJqgXtTxrQLxlfm5chCL12OFghjOnPnvSDb74IoHEn8EwPmMy2b6cPd8nsfNsA)].[[note](https://github.com/YimingXu1/multimodel-learning-notes/blob/main/Papers/CGMN.md)].

## Text, Image & Speech

- [ ] data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language.[[paper](https://scontent-amt2-1.xx.fbcdn.net/v/t39.8562-6/271974914_483120576492438_4239522333319653600_n.pdf?_nc_cat=107&ccb=1-5&_nc_sid=ae5e01&_nc_ohc=xM2q48LBPWMAX9sIWf-&_nc_ht=scontent-amt2-1.xx&oh=00_AT9vP0RN48dENdk7g4zDE_Nm0y92_94acOO25ZDFYRBL8A&oe=623D2311)].[[note](https://github.com/YimingXu1/multimodel-learning-notes/blob/main/Papers/data2vec.md)].

